{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Предобработка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_filter(data, days=2):\n",
    "    \n",
    "    \"\"\"Фильтрация данных до порогового значения\"\"\"\n",
    "    \n",
    "    # создаем таблицу с первым и последним действием юзера\n",
    "    min_max_user_time = data.groupby('user_id').agg({'timestamp': 'min'}) \\\n",
    "                            .rename(columns={'timestamp': 'min_timestamp'}) \\\n",
    "                            .reset_index()\n",
    "    \n",
    "    data_time_filtered = pd.merge(data, min_max_user_time, on='user_id', how='outer')\n",
    "    \n",
    "    # отбираем те записи, которые не позднее двух дней с начала учебы\n",
    "    learning_time_threshold = days * 24 * 60 * 60\n",
    "    data_time_filtered = data_time_filtered.query(\"timestamp <= min_timestamp + @learning_time_threshold\")\n",
    "    \n",
    "    assert data_time_filtered.user_id.nunique() == data.user_id.nunique()\n",
    "    \n",
    "    return data_time_filtered.drop(['min_timestamp'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def base_features(events_data, submission_data):\n",
    "    \n",
    "    \"\"\"Создание датасета с базовыми фичами: действия юзера \n",
    "    и правильные\\неправильные ответы\"\"\"\n",
    "    \n",
    "    # построим таблицу со всеми действиями юзеров\n",
    "    users_events_data = pd.pivot_table(data=events_data, values='step_id',\n",
    "                                   index='user_id', columns='action',\n",
    "                                   aggfunc='count', fill_value=0) \\\n",
    "                                   .reset_index() \\\n",
    "                                   .rename_axis('', axis=1)\n",
    "    \n",
    "    # таблица с колво правильных и неправильных попыток\n",
    "    users_scores = pd.pivot_table(data=submission_data, \n",
    "                              values='step_id',\n",
    "                              index='user_id',\n",
    "                              columns='submission_status',\n",
    "                              aggfunc='count',\n",
    "                              fill_value=0).reset_index() \\\n",
    "                              .rename_axis('', axis=1)\n",
    "    \n",
    "    # соединяем в один датасет\n",
    "    users_data = pd.merge(users_scores, users_events_data, on='user_id', how='outer').fillna(0)\n",
    "    \n",
    "    assert users_data.user_id.nunique() == events_data.user_id.nunique()\n",
    "    \n",
    "    return users_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def target(submission_data, threshold=40):\n",
    "    \n",
    "    \"\"\"Вычисление целевой переменной. Если юзер сделал 40 практический заданий,\n",
    "    то будем считать, что он пройдет курс до конца\"\"\"\n",
    "    \n",
    "    # считаем колво решенных заданий у каждого пользователя\n",
    "    users_count_correct = submission_data[submission_data.submission_status == 'correct'] \\\n",
    "                .groupby('user_id').agg({'step_id': 'count'}) \\\n",
    "                .reset_index().rename(columns={'step_id': 'corrects'})\n",
    "    \n",
    "    # если юзер выполнил нужное колво заданий, то он пройдет курс до конца\n",
    "    users_count_correct['passed_course'] = (users_count_correct.corrects >= threshold).astype('int')\n",
    "    \n",
    "    return users_count_correct.drop(['corrects'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_features(events_data):\n",
    "    \n",
    "    \"\"\"Создание временных фичей\"\"\"\n",
    "    \n",
    "    # добавление колонок с датами\n",
    "    events_data['date'] = pd.to_datetime(events_data['timestamp'], unit='s')\n",
    "    events_data['day'] = events_data['date'].dt.date\n",
    "    \n",
    "    # создаем таблицу с первым\\последним действием юзера и колвом уникальных дней, проведенных на курсе\n",
    "    users_time_feature = events_data.groupby('user_id').agg({'timestamp': ['min', 'max'], 'day': 'nunique'}) \\\n",
    "                        .droplevel(level=0, axis=1) \\\n",
    "                        .rename(columns={'nunique': 'days'}) \\\n",
    "                        .reset_index()\n",
    "    \n",
    "    # добавление колонки с разницей между первым и последним появлением юзера,\n",
    "    # другими словами, сколько времени юзер потратил на прохождение в часах\n",
    "    users_time_feature['hours'] = round((users_time_feature['max'] - users_time_feature['min']) / 3600, 1)\n",
    "    \n",
    "    \n",
    "    return users_time_feature.drop(['max', 'min'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def steps_tried(submission_data):\n",
    "    \n",
    "    \"\"\"Создание фичи с колвом уникальных шагов, которые пользователь пытался выполнить\"\"\"\n",
    "    \n",
    "    # сколько степов юзер попытался сделать\n",
    "    steps_tried = submission_data.groupby('user_id').step_id.nunique().to_frame().reset_index() \\\n",
    "                                        .rename(columns={'step_id': 'steps_tried'})\n",
    "    \n",
    "    return steps_tried"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_ratio(data):\n",
    "    \n",
    "    \"\"\"Создание фичи с долей правильных ответов\"\"\"\n",
    "    \n",
    "    data['correct_ratio'] = (data.correct / (data.correct + data.wrong)).fillna(0)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Создание датасетов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_df(events_data, submission_data):\n",
    "    \n",
    "    \"\"\"функция для формирования X датасета и y с целевыми переменными\"\"\"\n",
    "    \n",
    "    # фильтруем данные по дням от начала учебы\n",
    "    events_2days = time_filter(events_data)\n",
    "    submissions_2days = time_filter(submission_data)\n",
    "    \n",
    "    # создаем таблицу с базовыми фичами\n",
    "    users_data = base_features(events_2days, submissions_2days)\n",
    "    \n",
    "    # создаем целевую переменную\n",
    "    users_target_feature = target(submission_data, threshold=40)\n",
    "    \n",
    "    # создаем таблицу с временными фичами\n",
    "    users_time_feature = time_features(events_2days)\n",
    "    \n",
    "    # создаем фичи с попытками степов и долей правильных ответов\n",
    "    users_steps_tried = steps_tried(submissions_2days)\n",
    "    users_data = correct_ratio(users_data)\n",
    "    \n",
    "    # соединяем шаги\n",
    "    first_merge = users_data.merge(users_steps_tried, how='outer').fillna(0)\n",
    "    \n",
    "    # соединяем фичи со временем\n",
    "    second_merge = first_merge.merge(users_time_feature, how='outer')\n",
    "    \n",
    "    # присоединяем целевую переменную\n",
    "    third_merge = second_merge.merge(users_target_feature, how='outer').fillna(0)\n",
    "    \n",
    "    # отделяем целевую переменную и удаляем ее из основного датасета\n",
    "    y = third_merge['passed_course'].map(int)\n",
    "    X = third_merge.drop(['passed_course'], axis=1)\n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_test_df(events_data, submission_data):\n",
    "    \n",
    "    \"\"\"функция для формирования test датасета без целевой переменной\"\"\"\n",
    "    \n",
    "    # фильтруем данные по дням от начала учебы\n",
    "    events_2days = time_filter(events_data)\n",
    "    submissions_2days = time_filter(submission_data)\n",
    "    \n",
    "    # создаем таблицу с базовыми фичами\n",
    "    users_data = base_features(events_2days, submissions_2days)\n",
    "    \n",
    "    \n",
    "    # создаем таблицу с временными фичами\n",
    "    users_time_feature = time_features(events_2days)\n",
    "    \n",
    "    # создаем фичи с попытками степов и долей правильных ответов\n",
    "    users_steps_tried = steps_tried(submissions_2days)\n",
    "    users_data = correct_ratio(users_data)\n",
    "    \n",
    "    # соединяем шаги\n",
    "    first_merge = users_data.merge(users_steps_tried, how='outer').fillna(0)\n",
    "    \n",
    "    # соединяем фичи со временем\n",
    "    X = first_merge.merge(users_time_feature, how='outer')\n",
    "       \n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Загрузка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# загрузка тренировочного датасета\n",
    "events_data_train = pd.read_csv('./datasets/event_data_train.zip')\n",
    "submission_data_train = pd.read_csv('./datasets/submissions_data_train.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# создание тренировочного датасета с нужными фичами и целевой переменной\n",
    "X_train, y = create_df(events_data_train, submission_data_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# загрузка тестового датасета\n",
    "events_data_test = pd.read_csv('./datasets/events_data_test.zip')\n",
    "submission_data_test = pd.read_csv('./datasets/submission_data_test.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# создание тестового датасета\n",
    "X_test = create_test_df(events_data_test, submission_data_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Обучение модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_with_grid(train_data, y, size=0.20):\n",
    "    \n",
    "    \"\"\"Поиск наилучших параметров для RandomForest, обучаясь на тренировочной выборке.\n",
    "    Можно изменять или добавлять различные параметры. Может долго вычисляться.\"\"\"\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(train_data, y, test_size=size, random_state=42)\n",
    "    \n",
    "    param_grid = {'randomforestclassifier__n_estimators': range(20, 51, 3), \n",
    "                  'randomforestclassifier__max_depth': range(5, 14)}\n",
    "    \n",
    "    pipe = make_pipeline(RandomForestClassifier())\n",
    "    pipe.fit(X_train, y_train)\n",
    "    grid = GridSearchCV(pipe, param_grid=param_grid, cv=5, n_jobs=-1)\n",
    "    grid.fit(X_train, y_train)\n",
    "    print(f\"Наилучшие параметры: {grid.best_params_}\")\n",
    "    \n",
    "    ypred_prob = grid.predict_proba(X_test)\n",
    "    \n",
    "    roc_score = roc_auc_score(y_test, ypred_prob[:, 1])\n",
    "    score = grid.score(X_test, y_test)\n",
    "    print(f\"Правильность на тестовом наборе: {score:.2f}\")\n",
    "    print(roc_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_final(train_data, y, test_data, size=0.20):\n",
    "    \n",
    "    \"\"\"Финальное обучение на тренировочном датасете с лучшими параметрами и \n",
    "    получением predict_proba для тестового датасета с записей в csv файл\"\"\"\n",
    "    \n",
    "    test_data = test_data.sort_values('user_id')\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(train_data, y, test_size=size, random_state=42)\n",
    "    \n",
    "    pipe = make_pipeline(RandomForestClassifier(max_depth=7, n_estimators=40,  random_state=42))\n",
    "    pipe.fit(X_train, y_train)\n",
    "    \n",
    "    ypred_prob = pipe.predict_proba(X_test)\n",
    "    \n",
    "    roc_score = roc_auc_score(y_test, ypred_prob[:, 1])\n",
    "    score = pipe.score(X_test, y_test)\n",
    "    print(f\"Правильность на валид наборе: {score:.3f}\")\n",
    "    print(f\"Roc_auc_score на валид наборе: {roc_score:.5f}\")\n",
    "    \n",
    "    ypred_prob_final = pipe.predict_proba(test_data)\n",
    "    result = test_data['user_id'].to_frame()\n",
    "    result['is_gone'] = ypred_prob_final[:, 1]\n",
    "    result[['user_id', 'is_gone']].to_csv(f'my_predict_{roc_score:.5f}.csv', index=False)\n",
    "    print(f'Результы записанны в файл my_predict_{roc_score:.5f}.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Путем экспериментов выявленно, что фича hours уменьшает итоговую оценку. Выкидываем ее.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_feature(X_train, X_test, drop):\n",
    "    \n",
    "    \"\"\"Выкидываем выбранную фичу из обоих датасетов\"\"\"\n",
    "    \n",
    "    X_train_dropped = X_train.drop(drop, axis=1)\n",
    "    X_test_dropped = X_test.drop(drop, axis=1)\n",
    "    \n",
    "    return X_train_dropped, X_test_dropped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# удаляем фичу hours\n",
    "X_train_dropped, X_test_dropped = drop_feature(X_train, X_test, 'hours')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Правильность на валид наборе: 0.900\n",
      "Roc_auc_score на валид наборе: 0.88471\n",
      "Результы записанны в файл my_predict_0.88471.csv\n"
     ]
    }
   ],
   "source": [
    "# финальное обучение и предсказание\n",
    "random_final(X_train_dropped, y, X_test_dropped)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Финальный roc_auc на тестовых = 0.89087**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
